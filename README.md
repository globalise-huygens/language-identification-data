# language-identification-data

Data on languages and language scripts identified in the [Globalise VOC corpus](https://www.nationaalarchief.nl/onderzoeken/archief/1.04.02). Please [contact](https://globalise.huygens.knaw.nl/contact-us/) the Globalise Project if you would like to contribute a new language identification or offer a correction to an existing identification.

To date, we have identified c. 12,200 non-Dutch pages written (in part or in whole) in the following languages: French, Latin, English, Portuguese, Spanish, German, Italian, Malay (in Latin-script), and Danish. In addition, we have manually identified a further c. 180 pages written (in part or in whole) in several non-Latin script languages including Malay (in Arabic script), Chinese, Persian, Tamil, and Sinhalese as well as pages written in cipher. 

- [nondutchmono-pages.lang.tsv](https://github.com/globalise-huygens/language-identification-data/blob/main/latin-script-pages/nondutchmono-pages.lang.tsv) - automatically identified pages with a single, non-Dutch, Latin-script language (e.g. French); includes the relevant paragraph region texts
- [multilingual-pages.lang.tsv](https://github.com/globalise-huygens/language-identification-data/blob/main/latin-script-pages/multilingual-pages.lang.tsv) - automatically identified pages with a mix of Dutch and one or more non-Dutch, Latin-script languages (e.g. Dutch + English); includes the relevant paragraph region texts
- [pages.lang.tsv](https://github.com/globalise-huygens/language-identification-data/blob/main/latin-script-pages/pages.lang.tsv) - all automatically identified pages in Latin-script languages, including those only in Dutch; does not include the paragraph region text
- [non-latin.scripts.tsv](https://github.com/globalise-huygens/language-identification-data/blob/main/non-latin-script-pages/non-latin.scripts.tsv) - manually curated list of pages including text in one or more non-Latin (e.g. Arabic) scripts and languages (e.g. Persian), as well as pages in cipher; does not include the paragraph region text
- [unknown-pages.lang.tsv](https://github.com/globalise-huygens/language-identification-data/blob/main/latin-script-pages/unknown-pages.lang.tsv) - pages whose languages could not be automatically identified (e.g. blank pages, pages consisting of numerical tables); includes the relevant paragraph region texts
- [corrected-pages.tsv](https://github.com/globalise-huygens/language-identification-data/blob/main/corrections/corrected-pages.tsv) - manually curated list of corrections of all the automatically identified, Latin-script languages; does not include the paragraph region text

In the datasets above, languages and language scripts are identified by their three-letter [ISO-639-3](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes) and [ISO-15924](https://en.wikipedia.org/wiki/ISO_15924) codes.

### Methodology

The starting point for the automatic language identifications is a list of all lines of text in the transcribed Globalise corpus, their content in plaintext, and their layout region (e.g. paragraph, marginalia..). Each character on each line is first examined using a [character based language recognition model](https://github.com/pemistahl/lingua-rs/) for the presence of one or more instances of a pre-selected group of languages known to be present in the corpus (Dutch, French, Latin, English, Portuguese, Spanish, German, Italian, Malay, and Danish). This is then checked, on the word level, with a series of [language-specific lexical models](https://github.com/knaw-huc/globalise-tools/tree/main/pipelines/langdetect/lexicons). To improve the accuracy on Dutch text (for example, to better account for the widespread presence of foreign loan words in French and Latin) the Dutch lexical model was derived from the historical ground truth transcriptions of the Globalise VOC corpus. 

The predictions of the character and word based models are then [reconciled](https://github.com/knaw-huc/globalise-tools/blob/main/scripts/gt_classify_language.py), and a set of [heuristics](https://github.com/knaw-huc/globalise-tools/blob/main/scripts/gt_classify_language.py) are applied to assign one set of per-page language classifications from the individual, per-line classifications. Specifically, a non-Dutch language needs to have been identified on at least three lines or occur (this also applies to Dutch) on at least 25% of the total lines of text in the relevant layout region. These rules were applied to filter out, on the one hand, very short, non-Dutch phrases, but on the other hand be able to accomodate pages that are evenly split between Dutch and non-Dutch in parallel columns. Because on most pages only the paragraph layout region will contain significant amount of text, for the purpose of assigning the per-page language identifications, we have opted to set aside any text found in the remaining layout regions (marginalia, signatures, catch-word, page numbers etc.). The resulting language identifications of all text in the paragraph regions of all pages in the corpus can be found on [pages.lang.tsv](https://github.com/globalise-huygens/language-identification-data/blob/main/latin-script-pages/pages.lang.tsv).  

We expect, however, that researchers using the Globalise VOC corpus will want to distinguish between instances of Dutch pages with additional language content and documents written entirely in a single, non-Dutch language such as French or Malay. Additionally, we would like to make it as easy as possible for researchers to review for themselves the language identifications made by the automatic process described above. For this reason we have split the identifications collected in [pages.lang.tsv](https://github.com/globalise-huygens/language-identification-data/blob/main/latin-script-pages/pages.lang.tsv) into two derivative datasets, [nondutchmono-pages.lang.tsv](https://github.com/globalise-huygens/language-identification-data/blob/main/latin-script-pages/nondutchmono-pages.lang.tsv) and [multilingual-pages.lang.tsv](https://github.com/globalise-huygens/language-identification-data/blob/main/latin-script-pages/multilingual-pages.lang.tsv), including in them the text found in their paragraph regions. A third dataset, [unknown-pages.lang.tsv](https://github.com/globalise-huygens/language-identification-data/blob/main/latin-script-pages/unknown-pages.lang.tsv) lists all pages and paragraph region texts which we were not able to automatically identify. This could be for several reasons. By far the most common (over 120,000 instances) is insufficient text on the page (for example, in the case of entirely blank pages which were inserted between individual documents in an inventory). Other potential reasons for an 'unknown' classification include tabular pages with predominantly numerical content or pages that were inserted upside-down or sideways into an inventory.

Naturally, we are also very interested in identifying those pages that contain or consist entirely of text in non-Latin scripts and languages, such as Chinese, Persian, and Tamil or that were written in a secret cipher. Currently, we have not yet attempted to recognize these pages automatically, and are instead manually reviewing selections from [unknown-pages.lang.tsv](https://github.com/globalise-huygens/language-identification-data/blob/main/latin-script-pages/unknown-pages.lang.tsv) for erroneous HTR output that might suggest likely candidates. These pages are recorded in [non-latin.scripts.tsv](https://github.com/globalise-huygens/language-identification-data/blob/main/non-latin-script-pages/non-latin.scripts.tsv). We very much welcome further contributions to this list.

Finally, [corrected-pages.tsv](https://github.com/globalise-huygens/language-identification-data/tree/main/corrections) consists of a running list of corrections and amendments of all the automatically identified, Latin-script languages. Each time we publish a new release of the identified language datasets we incorporate the corrections on this list into it and reset the list to zero.

### Caveats

Based on our (thus far only qualitative) tests, the automatic language identification pipeline has excellent precision (i.e. there are very few cases of Dutch language texts classified as non-Dutch, and the identified non-Dutch texts are almost all correctly assigned to their respective languages) and good recall (i.e. there are still some instances of non-Dutch languages being misidentified as Dutch or unknown). For example, a series of pages in a document are correctly classified as e.g. French, but some pages in the French document are 'left out' and classified as Dutch. At the same time, some attributions are also inherently subjective. For example, whenn is there enough English text on a page for the page as a whole to be 'in English'? 90% of the text? 100%? It is precisely to give researchers the opportunity to apply their own criteria to questions such as these that we have provided additional datasets with monolingual non-Dutch, multilingual, and unknown classifications, alongside the relevant text extracts.     

### Resources

- [langdetect](https://github.com/knaw-huc/globalise-tools/tree/main/pipelines/langdetect) - Pipeline for language detection on the Globalise HTR output
- [lingua-cli](https://github.com/proycon/lingua-cli) - A command-line tool for language detection, it is a simple wrapper around the [lingua-rs](https://github.com/pemistahl/lingua-rs/) library for Rust

### Credits

- Bram Buitendijk, HuC Digital Infrastructure Department
- Maarten van Gompel, HuC Digital Infrastructure Department
- Arno Bosse, HuC Digital Infrastructure Department; Globalise Project
