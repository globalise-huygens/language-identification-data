# language-identification-data

Data on languages and language scripts identified in the Globalise VOC corpus. Please [contact the Globalise Project](https://globalise.huygens.knaw.nl/contact-us/) if you would like to contribute a new language identification or offer a correction to an existing identification.

To date, we have identified pages written (in part or in whole) in the following languages: Dutch, French, Latin, English, Portuguese, Spanish, German, Italian, Malay (in Latin-script), and Danish. In addition, we have manually identified pages written (in part or in whole) in several non-Latin script languages including Malay (in Arabic script), Chinese, Persian, Tamil, and Sinhalese.

- [nondutch-pages.lang.tsv](https://github.com/globalise-huygens/language-identification-data/blob/main/latin-script-pages/nondutch-pages.lang.tsv) - automatically identified pages with a single, non-Dutch, Latin-script language (e.g. French); includes the relevant paragraph region texts
- [mixed-pages.lang.tsv](https://github.com/globalise-huygens/language-identification-data/blob/main/latin-script-pages/mixed-pages.lang.tsv) - automatically identified pages with a mix of Dutch and one or more non-Dutch, Latin-script languages (e.g. Dutch + English); includes the relevant paragraph region texts
- [pages.lang.tsv](https://github.com/globalise-huygens/language-identification-data/blob/main/latin-script-pages/pages.lang.tsv) - all pages, including those only in Dutch, with all automatically identified Latin-script languages; does not include any text
- [non-latin.scripts.tsv](https://github.com/globalise-huygens/language-identification-data/blob/main/non-latin-script-pages/non-latin.scripts.tsv) - manually curated list of pages including text in one or more non-Latin scripts and languages (e.g. Chinese); does not include any text 
- [unknown-pages.lang.tsv](https://github.com/globalise-huygens/language-identification-data/blob/main/latin-script-pages/unknown-pages.lang.tsv) - pages whose languages could not be automatically identified (e.g. blank pages, pages with consisting of numerical tables); includes the relevant paragraph region texts
- [corrected-pages.tsv](https://github.com/globalise-huygens/language-identification-data/tree/main/corrections) - manually curated list of corrections to all the above categories; does not include any text

Please see below for more details on the methodology used to define and create these datasets.

### Methodology

The starting point for the automatic language identifications is a list of all lines of text in the transcribed Globalise corpus, their content in plaintext, and their layout region (e.g. paragraph, marginalia..). In brief, each character on each line is first examined using a [character based language recognition model](https://github.com/pemistahl/lingua-rs/) for the presence of one or more instances of a pre-selected group of languages known to be present in the corpus (Dutch, French, Latin, English, Portuguese, Spanish, German, Italian, Malay, and Danish). Please note that the HTR transcription platform used by Globalise can only recognize and output text written in Latin-script. This is then checked, on the word level, with a series of [language-specific lexical models](https://github.com/knaw-huc/globalise-tools/tree/main/pipelines/langdetect). To improve its accuracy (for example, to better account for the presence of foreign loan words in e.g. French or Latin) the Dutch lexical model was derived from the historical, ground truth transcriptions of the [Globalise VOC corpus](https://www.nationaalarchief.nl/onderzoeken/archief/1.04.02). The predictions of the character and word based models are then reconciled, and a set of heuristics are applied to assign one set of per-page language classifications from the individual, per-line classifications. As a practical matter, since only the paragraph layout region will contain significant amount of text, for the purpose of assigning the per-page language identifications, we have opted to set aside the text found in the remaining layout regions (marginalia, signatures, catch-word, page numbers etc.). Finally, because we suspect that researchers using the Globalise VOC corpus will want to distinguish between instances of Dutch pages with additional language content (e.g. parallel columns of Dutch and English text) and documents written entirely in e.g. French or Malay (i.e. transcribed in Latin-script) we have split these identifications into two datasets ([nondutch-pages.lang.tsv](https://github.com/globalise-huygens/language-identification-data/blob/main/latin-script-pages/nondutch-pages.lang.tsv) and [mixed-pages.lang.tsv](https://github.com/globalise-huygens/language-identification-data/blob/main/latin-script-pages/mixed-pages.lang.tsv)). A third dataset,    
